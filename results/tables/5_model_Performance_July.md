TODO Zu hasue:
- 1. Prio NIMO lernen: alles!
- 2. Prio Synthetic Data generation lernen
- 3. Zusammenfassung beenden bei Plots
- erklÃ¤re in allen synthetic data weshalb viel features weshalb wenig linearity etc
- variant und baseline differences

TODO Aktuell:
- mache neu Neuralnet mit diesen sachen, mit notes updaten.
- paralle beide NN laufen lassen um unterschiedmerken ob es besser ist als alte.
- nach scenario update nochmals anschauen ob scenraien besser sind, ebssere outcome
- dann nimo besser verstehen
- interpretatio fÃ¼r alle scenarios machen

Frage:
- non linearity und rho sind auf alle features oder random auf gewisse? wie wird data generiert? wichtig zu wissen



# Roadmap
- different approaches of NIMO
- snythetic data more (how to create synthetic data), see how they did in nimo_Official)
  - test with different synthetic data
- learn about theory, about the steps, search for roadmap in AIchat: Logistic Regression & the Forwardâ€‘Pass
- difference in NIMO and MyVersion
- make some experiments with different implementations (include original nimo)
- find out why nimo_official went not good
- rename nimo_official or our nimo
- listen to meeting with assistants
- find out differences between nimoErdem and nimoOfficial, like one hot and not on-hot (for table search for: Offizielles Repo<br/>(AdaptiveRidgeLogisticRegression))
- volker and assitanat asked if i have one nn or multiple
  - since it makes sense because of time consumption
  - what did i do?
  - are both ways possible or not?
- make a summary of inmportant stuff volker gave till now:
  - like pair tsting, synthetc dataset, subsampling, normalization etc. maybe look into the voice recordings again
- Check the audio with volker, try to answer all questions
- for volker create a sketch / entwurf of the algorithmus, how we do masking etc
  - Vmapâ€‘Trick for masking
  - verstehe im code adaptive ridge + sparsity
- Can I also make Lasso implementation better? additional steps to have full potential of lasso?


TOStudy:
- IRLS & Working Response lernen, Theory
- Adaptive Ridge Theory lernen
- in NIMO haben wir ja designmatrix.
  - Designâ€Matrix genau die richtigen Interaktionsâ€‘ und NichtlinearitÃ¤tsÂ­terme (oder gar die wahren Koeffizienten) vorliegen hÃ¤tte
  - also erlernt man dort nichtlinearitÃ¤ten und interaktionen zwischen den features?
  - NIMONetâ€‘Variant (grÃ¼n gepunktet) hÃ¤lt sich deutlich stabiler bei F1â€¯â‰ˆâ€¯0.93â€“0.95. Es nutzt offenbar seinen sparsamen Maskierungsâ€‘Prior
  - lernen marskierungs prior? was meint man damit


TOOD Improtant:
- look for synthetic data: how it is generated? find appropriate ways to do it
- understand the code of nimo
- implement multiple version of nimo

TODONEW:

Prio 1:
- learn different synthetic dataset what are they making, try to understand and explain why which makes sense
- understand nimo
- make snythetic data really step by step. first just rho, then just supports, than combination etc. to really see which scenraio for which case

Prio 2:
- chekc if baseline is the same like Official
- new version of nimos that makes sense

understand NIMO:
- Nonlinear structure: 
  - Standard sparse linear models (like Lasso) can only pick linear main effects. 
  - NIMONet injects a small neural â€œcorrectionâ€ network so that once features are masked in or out, 
  - it can still learn smooth nonlinear transformations of those selected inputs.
- Interactions:
  - Beyond just sinusoids or quadratics, you often want to capture featureâ€“feature interactions (e.g. Xâ‚€Â·Xâ‚, or more complex patterns). 
  - NIMONet uses a groupâ€‘sparse mask over groups of neurons, effectively learning which combinations (or higherâ€‘order interactions) 
  - to include, while still enforcing overall sparsity for interpretability.



Synthetic Data:
- if there is multicollinearity we cannot distinguish between features coefficient well. So we cannot do sparsity right. selected features not good
  - there is a way for diagnosis: setting up regression model, take the features as dependent value and try to  predict the feature based on other features. thereby we dont need the feature anymore
  - it is overtermined
  - with tolerance and maybe VIF variance inflation vector better to select the features

TODO snythetic:
- Lasso under high correlation:
  - Signalâ€toâ€noise: If the true Î²â€™s are strong and noise is mild, lasso will reliably pick a handful of highly predictive featuresâ€”
  - even if theyâ€™re somewhat collinear.
  - so we need to add more noise maybe then it will not be able to get it. but just rho ist not enough
- try to adapt senrario findo out which makes sense "lasso here high yes makes sense, nimo not why? we have to adapt nimo?"
- make matrix with true not true selected


Questions to ask to volker:
- can we use multicollinearity technique to have better sparsity? if the data is multicollinear?
- i wanted to plot with synthetic data each scenrarios where lasso is not performing good. but somehow it is almosty everytime performing good. Why?
- can we make nimo so good that it can also act like normal NN? for isntance in cases with nonlinear datasets where NN performs quite good
- "If yâ‰ˆsin(Xâ‚€), then plotting Xâ‚€ vs. y shows a wave. Unless your model â€œknowsâ€ to include sin(Xâ‚€) or to curveâ€fit, it will only try straight lines."
  - do we add these nonlinear cases in nimo? like these beginning where we have activation like tanh and sinus is it this?
- we just have RBF custom, does this make sense? should we include other customs? what makes sense?
  - can we even have nice performance by rbf with lasso or nimo, is it even possible?
  - should we not have with NN in rbf settings very good performance?
  - for instance applying this: 
    - "Lasso or plain logistic on raw X sees zero signalâ€”theyâ€™d predict at chance. A kernel method (e.g. RBFâ€kernel SVM) or a net that can construct those bump features can succeed."
- What to do for nimo to suceed in these situations:
  - **Correlated** features demand either grouping (elastic net) or more robust selection (stability selection) if you care about support.
  - **Interactions** need either manual feature crossing, tree methods (RF/GBM), or nets.
  - **Heavyâ€tailed** noise calls for robust losses (Huber, quantile) or outlierâ€‘resistant modeling.
  - **RBFâ€like structure** calls for kernel machines or architectures able to learn localized features.
  
Main Question Volker Roth:
- are my scenarios good? if so i would focus now on Neural Net and NIMO to adapt and make it better?
- why do i have always same F1 for lasso?
  - should i always include statistical pair testing also later? or at the end?
- for IRLS step should we apply conjugate gradient descent


TODO General:
- NeuralNet kann prinzipiell Interaktionen lernen, ist hier aber zu schwach regularisiert und liefert nur mÃ¤ssige Leistung.
  - NN verbessern damit es besser performed 
  - handful of hidden units (and no explicit regularization toward those shapes) it only partially recovers the two nonlinear terms
  - Weil das Netz ohne explizite PeriodizitÃ¤tsâ€‘Features auf rohe X-Werte trainiert, muss es erst Die Sinusâ€Beziehung aus rohen Merkmalen lernen, 
  - was bei so hoher Frequenz schwierig ist und zu InstabilitÃ¤t fÃ¼hrt.
  - FÃ¼r SÃ¤gezahn:
    - piecewiseâ€ReLU/Maxoutâ€Layer im NN verwenden, um Kanten besser scharf darzustellen.
- Baseline NIMO, Prior problem eventuell?
  - NIMOâ€‘Baseline ohne sparsityâ€induzierenden Prior muss sich in dieser Aufgabe geschlagen geben.

- Nimo_Variant, oder lieber bei NimoNew dmait man einen Unterschied hat:
  - NIMOâ€‘Baseline ohne sparsityâ€induzierenden Prior muss sich in dieser Aufgabe geschlagen geben.
  - Architektur anpassen 
    - ErhÃ¶he die Tiefe oder Breite deines Netzes (z.â€¯B. 3â€“4 Hiddenâ€‘Layer mit je 64â€“128 Units statt 2Ã—32). 
    - FÃ¼ge Dropout oder BatchNorm hinzu, damit es auch bei hoher Korrelation stabil lernt. 
  - Explizite Basisâ€‘Erweiterung im NIMOâ€‘Maskâ€‘Layer 
    - Bau in deinem Maskâ€‘ingâ€‘Mechanismus nicht nur Rawâ€‘X ein, sondern auch XÂ² und XÂ³ als separate KanÃ¤le. 
    - Dann lernt NIMO direkt, welche dieser Polynome wichtig sind. 
  - Lossâ€‘Term fÃ¼r NichtlinearitÃ¤tsâ€‘Detektion 
    - ErgÃ¤nze im NIMOâ€‘Objective einen kleinen Penaltyâ€‘Term, der gezielt hohe Gewichte auf quadratische KanÃ¤le fÃ¶rdert, wenn deren Aktivierung die F1 verbessert. 
  - Hyperparameterâ€‘Suche 
    - Justiere Learningâ€‘Rate, Regularisierung (L2), und sparsityâ€‘Gewicht fÃ¼r das Maskâ€‘Prior â€“ manchmal hilft schon ein stÃ¤rkerer Sparsityâ€‘Term, damit NIMO sauber die wahren Polynomâ€‘KanÃ¤le isoliert. 
  - Featureâ€‘Normalisierung 
    - Gerade bei xÂ³ kann der Wertebereich stark abweichen â€“ minâ€“max oder Standard-Scaling auf jede Basisfunktion, bevor sie ins Netz gehen.
- fÃ¼r RBF, weiss nicht ob nÃ¶tig ist. ( Szen. I)
  - Kernelâ€‘Trick/NIMOnetâ€‘Erweiterung: ErgÃ¤nze im Maskâ€‘Layer nicht nur Rawâ€‘X, sondern auch solche GauÃŸâ€‘Bumps. 
  - So lernt NIMO direkt, welche Center relevant sind, und kÃ¶nnte die konstant besseren Basisâ€‘Features auswÃ¤hlen.
- Nimo Szen K 
  - erweitere dein Netz um multiplicative Layers.
- Nimo Szen L
  - FÃ¼r NeuralNet und NIMO mÃ¼sstest du entweder die Sparsityâ€‘Prior stÃ¤rker gewichten oder architektonisch Multiplikationsâ€‘ bzw. Sinusâ€‘Netzwerke (Fourierâ€‘Features) einbauen.



Presentation on Monday to Volker:
- Make one full everything results and one just nimo and neuralt net. or change the code that you can de-seleect some methods
- explain what i changed in nimo_official, which file i took and which stuff i changed (gpu etc)
- Main goal more exact feature selection higher F1
  - ich versuche NErual Net und Nimo beides parallel zu verbessern. Assitant stutzig gemacht gesagt NN immer gleich gut wie Nimo
  - haputunterschiede variant und baseline
- GOAL:
  - A: wie erwartet in Ordnung
  - B: Nimo variant should choose less features since it has the NN included
    - Kein Modell kommt hier auf exakt 5 ausgewÃ¤hlte Features (die beiden Interaktionsterme plus die 3 linearen Hauptterme). 
    - Das ist aber erwartbar, weil wir den Designâ€‘Matrix nicht um explizite Produktâ€‘Spalten erweitert haben.
  - C: 
  - D:
  - E:
    - If you wanted to push this even harder, you could drop the degrees of freedom of the Studentâ€‘t even lower (say df=1 or 2), or throw in a larger variance Gaussian on top â€” 
    - that will create more catastrophic outliers and widen the spread in your NNâ€™s F1 curve even further, while lasso stays rockâ€‘solid.
    - Noise problem for NN and NIMO
    - **Lasso** bleibt stabil, weil es ohne grosse Anpassung an einzelne Ausreisser selektiert. 
    - **NeuralNet** schwankt stark, weil die Heavyâ€‘Tails die Logits herumreissen und es keine explizite Robustheitsâ€‘Regularisierung gibt. 
    - **NIMONet** ist dazwischen: Es hÃ¤lt sich besser als das reine NN, aber kann den Ausreissern nicht vÃ¶llig entkommen.
    - Fazit: â€Ausreisserâ€‘lastiger, heavyâ€‘tailed Noise bricht unregulierte HochkapazitÃ¤tsâ€‘Modelle, wÃ¤hrend sparsere Priors sie stabilisieren.â€œ
  - G:
    - Dieses Szenario deckt genau die SchwÃ¤che linearer/Sparsityâ€‘Methoden ab und zeigt den Nutzen von Modellen, die echte Interaktionsâ€‘Terms lernen oder approximieren kÃ¶nnen.
  - H:
    - Er demonstriert, dass ohne Featureâ€‘Augmentation auch starke Regularisierung und maskierte Netze nicht perfekt den wahren quadratisch/kubischâ€‘getriebenen Support zurÃ¼ckfinden. 
    - Wenn du aber zeigen willst, dass NIMO gegenÃ¼ber Lasso einen klaren Vorteil hat, mÃ¼sstest du dem Netzwerk mehr KapazitÃ¤t oder â€” noch besser â€” explizit polynomielle Basisfunktionen in den Input geben.
  - I:
    - Szenario I demonstriert sehr schÃ¶n, dass du ohne die richtigen nichtâ€‘linearen Basisfunktionen (hier RBFâ€‘Glocken) an die Performanceâ€‘Grenze stosst.
  - J:
    - Dieses Szenario ist ein guter â€Stressâ€‘Testâ€œ fÃ¼r Nichtâ€‘lineare mit kniffligen Kanten
    - Zwar keine Methode komplett versagt, aber auch keine perfekt die harten Nichtâ€‘LinearitÃ¤ten meistert
  - K:
    - ohne explizite Designâ€‘Matrix sind fÃ¼r lineare Methoden (Lasso) ein totales Noâ€‘Go, daher ihr konstantes, aber niedriges Niveau.
    - ohne explizite Interaktionsterms alle Verfahren â€“ linear, NN und NIMO â€“ deutliche Schwierigkeiten haben, 
    - und nur Lasso stabil ein mittleres Niveau erreicht, weil es einfach konstant das Korrelationsniveau abschÃ¶pft
  - M:
    - NIMO dort am stÃ¤rksten punktet, wo **Interaktion + NichtlinearitÃ¤t + Korrelation + AusreiÃŸer** zusammenkommen â€“ 
    - klassische FÃ¤lle, in denen weder reines Lasso noch ein undifferenziertes NeuralNet ideal sind.



To learn for volker:
- rho is the correlation hyperparameter

Steps:
1. get results for interpretability/sparsity
2. learn theory
3. see difference from Nimo official in code
4. make nice experiemnts test (differnt nimos, different datasets/synthetic)
5. find out why nimo_Official is not performing good
   - maybe because no gpu? maybe because some files missing the are in pipeline of nimo_official

# Notes:
- What i did for changes in nimo official:
   - deleted cuda stuff, gpu stuff 
   - the if None also changed

Stuff to learn:
- forwardâ€pass, the IRLS + adaptiveâ€ridge updates, the nonlinearity, the zeroâ€mean correction or the groupâ€Lasso proximal step 

# NIMO Aktuell

- In deiner aktuellen NIMO-Implementierung fÃ¼hrst du im IRLS-Schritt zwar eine Ridge-Regression durch
   - das ist aber eine klassische, nicht-adaptive Ridge mit festem Î» fÃ¼r alle Koeffizienten.
   - ich nutze eine einheitliche Ridge (ğœ†*ğ¼), also keine adaptive Gewichtung. 
   - LÃ¶sung: Um das einzubauen, mÃ¼sstest du nach jedem IRLS-Schritt die w_j (zB 1/beta_j^gamma) neu berechnen und in Matrix A als diag (ğœ†*w) statt (ğœ†*ğ¼ einsetzen)


# Questions
- do i need to have GPU or something to run NIMO?
- i have always 0.4 f1 score. waht do you think is it because of dataset, all emthdos aournd 0.4
- How is nimo differentiating between  logistic and continuous target variables?


# TODO Code
- Mini-Batches
- automate Hyparparameter selection
- Early-Stopping & Convergence-Criterion
  - find out if it makes sense to have an early stopping. make debugs and see the trend of loss, if at some point it converges or even gets worse
- Tanh/Sinus-Activations & Dropout:
  - Du hast schon Tanh() und SinAct(). Du kÃ¶nntest testen, ob es hilft, das Sinus-Layer vor oder nach der zweiten FC zu setzen, oder mit verschiedenen Skalen (z.B. sin(Î± x)) zu spielen. 
  - Ebenfalls kÃ¶nntest du experimentieren, ob ein zusÃ¤tzliches Dropout in der ersten Schicht (statt nur einmal) Ãœberanpassung weiter vermeidet.
- Adaptive-Ridge (Adaptive-Lasso) weiter verfeinern:
  - Im Paper empfehlen sie, das Î³-Update (w = 1/|Î²|^Î³) auch innerhalb des IRLS-Loops etwas anzupassen (z.B. Î³ â† Î³Â·c oder mit kleinem Learning-Rate), um die Sparsity schÃ¤rfer zu kontrollieren.
- Logging von Training und Î²-Werten:
  - Sammle nach jeder Iteration den Wert von âˆ¥Î²âˆ¥â‚€ (Anzahl non-zero Î²) und âˆ¥Î²âˆ¥â‚‚, und plotte den Verlauf, um zu sehen, wie schnell deine adaptive-Lasso-Sparsity einsetzt.

# Future topcis to study

- Meerkat Statistics videos: logit/link-function,bernoulli, binomial, glm videos, NN vs Logistic Regresion  

# Future, reduce scenarios:

Du hast jetzt 13 sehr unterschiedliche Settings â€“ von rein linearen (A) Ã¼ber Interaktionen (B, G, K), NichtlinearitÃ¤ten (C, F, J), Korrelation (H, D), RBFâ€‘Features (I), Heavyâ€‘tailed Noise (E, M) bis hin zu Highâ€‘Dim/Lowâ€‘N (L). Das deckt de facto den gesamten Spektrum ab.


## Was gut ist

- Jeder Hauptâ€‘Effekt (Lineare vs. Interaktive vs. Nichtlineare vs. Kernelâ€‘Manifold) ist vertreten. 
- Du testest robuste (Studentâ€‘t), schwere AusreiÃŸerâ€‘ und Standardâ€‘Gaussianâ€‘Rauschen. 
- Du hast Szenarien mit und ohne Featureâ€‘Korrelation (Ï von 0 bis 0.95). 
- Der Extremfall n<p (â€œLâ€) greift typische Genomâ€‘/Textâ€‘Daten ab.

## Wo sich Dinge Ã¤hneln
- E vs. M: Beide heavyâ€‘tailed Noise, M hat zusÃ¤tzlich Interaktionen + Polynom. Du kÃ¶nntest E (nur heavyâ€‘tailed) streichen, wenn dir M schon zeigt, dass AusreiÃŸer zum Problem werden. 
- H vs. D: Beide korrelierte Polynomâ€‘Terms + D hat noch eine Sinusâ€‘Komponente und stÃ¤rkere Korrelation + Interaktionen + heavy noise. D ist der superset, H also redundant? Wenn dir H nur einen monotoneren Einstieg erlaubt, kÃ¶nnte man es beibehalten â€“ ansonsten reicht D. 
- C vs. F vs. J: Alle drei sind â€reineâ€œ NichtlinearitÃ¤ten. C kombiniert Sin/Square/Cube; F nur hochfrequente Sinusâ€‘â€œVersteckerâ€ (X selbst bleibt linear); J eine SÃ¤gezahnâ€‘Funktion. Technisch deckt C schon alles ab (mehr Basisfunktionen), F und J sind also eher Edgeâ€‘Cases. Wenn es dir primÃ¤r um die Herausforderung â€lassoniveausâ€œ geht, kÃ¶nntest du F und/oder J rausnehmen.

## Empfehlung
- Coreâ€‘Gruppen beibehalten: A, B, C, D, G, I, L, M 
- Optional (je nach Fokus): E, H, F, J, K 
  - Wenn du Zeit/Plotâ€‘Budget sparen willst, streiche E (nur Noise), H (nur Polynom+Corr), F (sinâ€‘highfreq) und J (sawtooth) â€“ C deckt den nichtlinearen Teil ab, D deckt Korrelation+Noise ab. 
  - Falls du jeden Effekt isoliert zeigen mÃ¶chtest, behalte alle.

Kurz: FÃ¼r eine schlanke PrÃ¤sentation reichen etwa 8â€“9 Szenarien. Alles darÃ¼ber hinaus ist zwar didaktisch interessant, wirkt aber schnell redundant.